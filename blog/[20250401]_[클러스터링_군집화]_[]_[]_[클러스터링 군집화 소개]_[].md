# 클러스터링(군집화) 분석 실습

### K-Means 클러스터링

1. k-means(k=2) 모델 생성 및 학습
```python
from sklearn.cluster import KMeans
```
k=2로 클러스터링 모델 생성
model = KMeans(n_clusters=2, random_state=123)

### 모델 학습
model.fit(scaled_df)
2. 군집 레이블(label) 생성 및 데이터에 추가
```python
# label 컬럼 추가
scaled_df['label'] = model.predict(scaled_df)
# 결과 확인
scaled_df
```
3. 군집 시각화 및 중심점 표시
```python
import seaborn as sns

# 데이터 포인트 시각화
sns.scatterplot(x=scaled_df['total_buy_cnt'], y=scaled_df['total_price'], hue=scaled_df['label'], s=200, palette='bright')

# 군집 중심점 표시
centers = model.cluster_centers_
sns.scatterplot(x=centers[:, 0], y=centers[:, 1], color='black', alpha=0.8, s=400)
```

최적의 k값 찾기 (Elbow Method)
1. label 컬럼 제거
```python
scaled_df = scaled_df.drop(['label'], axis=1)
```

2. inertia 값 저장 및 시각화
```python
inertias = []

for k in range(1, 16):
    model = KMeans(n_clusters=k, random_state=123)
    model.fit(scaled_df)
    inertias.append(model.inertia_)

# Elbow Method 시각화
sns.lineplot(x=range(1, 16), y=inertias, marker='o')
```
inertia 값: 클러스터 내 거리를 의미하며, 작을수록 클러스터링이 잘 된 것.

계층적 클러스터링 (Hierarchical Clustering)
1. 덴드로그램(Dendrogram) 생성
```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# 거리 계산 (ward method 사용)
model = linkage(scaled_df, 'ward')

labelList = scaled_df.index

# 덴드로그램 시각화
plt.figure(figsize=(16,9))
plt.style.use("default")
dendrogram(model, labels=labelList)
plt.show()
```

2. 클러스터 수 설정 및 레이블 할당
```python
from scipy.cluster.hierarchy import cut_tree

cluster_num = 5

# 고객별 클러스터 라벨 구하기
scaled_df['label'] = cut_tree(model, cluster_num)

# 클러스터별 데이터 수 확인
pd.DataFrame(scaled_df['label'].value_counts())
```

3. 군집 시각화
```python
sns.set(style="darkgrid", rc={'figure.figsize':(16,9)})

sns.scatterplot(x=scaled_df['total_buy_cnt'], y=scaled_df['total_price'], hue=scaled_df['label'], s=200, palette='bright')
DBSCAN (Density-Based Spatial Clustering)
```

1. 반달 모양 데이터셋 생성
```python
from sklearn.datasets import make_moons
import numpy as np

n_samples = 1000
np.random.seed(3)

X, y = make_moons(n_samples=n_samples, noise=0.05)
df = pd.DataFrame(X)

# 데이터 시각화
plt.figure(figsize=(16,9))
sns.scatterplot(x=df[0], y=df[1], marker='o', s=200)
```

2. DBSCAN 모델 학습 및 레이블 생성
```python
from sklearn.cluster import DBSCAN

eps = 0.1
min_samples = 5

model = DBSCAN(eps=eps, min_samples=min_samples)
model.fit(df)

df['dbscan_label'] = model.labels_

# 결과 시각화
plt.figure(figsize=(16,9))
sns.scatterplot(x=df[0], y=df[1], hue=df['dbscan_label'], s=200)
K-Means 클러스터링 (타원형 데이터)
```

1. 데이터 생성 및 변형
```python
from sklearn.datasets import make_blobs

n_samples = 500
centers = 4
cluster_std = 0.75
random_state = 13

data, clusters = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=random_state)

# 데이터 타원형 변형
tf = [[0.6, -0.6], [-0.4, 0.2]]
data_tf = data @ tf
df = pd.DataFrame(data_tf)

# 데이터 시각화
sns.scatterplot(x=df[0], y=df[1], alpha=0.7, edgecolor="k", s=100)
```

2. k-means 모델 학습 및 결과 시각화
```python
model = KMeans(n_clusters=4, random_state=123)
model.fit(df)

df['kmeans_label'] = model.predict(df)

centers = model.cluster_centers_

# 시각화
sns.scatterplot(x=df[0], y=df[1], hue=df['kmeans_label'], palette='rainbow', alpha=0.7, s=200)
sns.scatterplot(x=centers[:,0], y=centers[:,1], color='black', alpha=0.8, s=100)
GMM (Gaussian Mixture Model)
```

1. 기존 라벨 제거
```python
df = df.drop(columns=['kmeans_label'], axis=1)
```

2. GMM 모델 학습 및 군집화
```python
from sklearn.mixture import GaussianMixture

n_components = 4
random_state = 10

model = GaussianMixture(n_components=n_components, random_state=random_state)

model.fit(df)

df['gmm_label'] = model.predict(df)

# 결과 시각화
sns.scatterplot(x=df[0], y=df[1], hue=df['gmm_label'], palette='rainbow', alpha=0.7, s=100)
```

정리 요약
KMeans: 중심 기반 클러스터링, 빠르고 직관적

Hierarchical Clustering: 덴드로그램 기반, 계층적 관계 파악 가능

DBSCAN: 밀도 기반, 이상치 감지에 유리

GMM: 데이터가 가우시안 분포를 따른다고 가정

참고: 데이터 특성(모양, 밀도 등)에 따라 적절한 클러스터링 기법을 선택해야 합니다.